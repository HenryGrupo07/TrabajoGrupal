{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Se importan librerias para leer uno de los archivos Json provistos y hacer el filtro de la categoria \n",
        "## que vamos a utilizar: \"industria Hotelera de EEuu\". Trabajamos con el dataset \"Sitios\"\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import ast\n",
        "import sys\n",
        "import re\n",
        "import reverse_geocoder as rg\n",
        "import numpy as np\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def get_urls(url):\n",
        "    rutas = []\n",
        "    for folder in os.walk(url):\n",
        "        rutas.append(folder)\n",
        "    df = pd.DataFrame(rutas, columns =['carpeta', 'sub_carpeta', 'archivos'])\n",
        "    \n",
        "    if df[\"archivos\"][0] == []:\n",
        "        df.drop(0,inplace=True)\n",
        "    df = df.explode(\"archivos\")\n",
        "    df.reset_index(drop=True,inplace=True)\n",
        "    df[\"carpeta\"] = df[\"carpeta\"].apply(lambda x: x.replace(\"\\\\\",\"/\"))\n",
        "    df[\"ruta\"] = df[\"carpeta\"] + \"/\" + df[\"archivos\"]\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_reviews_url_info(serie_url, filtro, guardar=False):\n",
        "    json_concat = pd.DataFrame()\n",
        "    for i in serie_url:\n",
        "        json_x = pd.read_json(i,lines=True)\n",
        "        json_x[\"url_origen\"] = i\n",
        "        json_x = json_x[json_x[\"gmap_id\"].isin(filtro[\"gmap_id\"])]\n",
        "        json_concat = pd.concat([json_concat,json_x])\n",
        "    json_concat[\"etl_timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    json_concat[\"etl_timestamp\"] = pd.to_datetime(json_concat[\"etl_timestamp\"])\n",
        "    json_concat.reset_index(drop=True,inplace=True)\n",
        "    if guardar == True:\n",
        "        json_concat.to_csv(\"d:/Henry/Proyecto Final/Dataset_generados/csv_{}.csv\".format(datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
        "    return json_concat\n",
        "\n",
        "\n",
        "def only_dict(d):\n",
        "    '''\n",
        "    Convert json string representation of dictionary to a python dict\n",
        "    A = json_normalize(df['columnA'].apply(only_dict).tolist()).add_prefix('columnA.')\n",
        "    '''\n",
        "    return ast.literal_eval(str(d))\n",
        "\n",
        "def list_of_dicts(ld):\n",
        "    '''\n",
        "    Create a mapping of the tuples formed after \n",
        "    converting json strings of list to a python list\n",
        "    B = json_normalize(df['columnB'].apply(list_of_dicts).tolist()).add_prefix('columnB.pos.') \n",
        "    '''\n",
        "    return dict([(list(d.values())[1], list(d.values())[0]) for d in ast.literal_eval(ld)])\n",
        "\n",
        "def formatear_columnas(df_columnas,df_to_merge):\n",
        "    '''Esta funcion sirve para darle formato a los nombres de las columnas dinamizadas y luego hacer un merge con otro dataframe por su mismo indice'''\n",
        "    df_columnas.columns = df_columnas.columns.str.strip()\n",
        "    df_columnas.columns = df_columnas.columns.str.replace(\" \",\"_\")\n",
        "    df_columnas.columns = df_columnas.columns.str.replace(\"&\",\"\")\n",
        "    df_columnas.columns = df_columnas.columns.str.replace(\"__\",\"_\")\n",
        "    df_final = df_to_merge.merge(df_columnas,left_index=True, right_index=True,how=\"left\")\n",
        "    return df_final\n",
        "\n",
        "def get_gmap_id(url_series):\n",
        "    df = pd.DataFrame()\n",
        "    for url in url_series:\n",
        "        df_json = pd.read_json(url, lines=True)\n",
        "        df_json = df_json[[\"name\",\"gmap_id\",\"category\"]]\n",
        "        df_categorias = df_json.explode(\"category\")\n",
        "        df_categorias[\"category\"] = df_categorias[\"category\"].str.lower()\n",
        "        df_categorias = df_categorias[(df_categorias[\"category\"].str.contains('hotel|resort|inn|motel|lodg',regex = True))== True] #agregar lodge ganamos 2k sitios mas\n",
        "        df = pd.concat([df,df_categorias])\n",
        "    return df\n",
        "\n",
        "def reduccion_categoria(categoria):\n",
        "    if \"dinner\" in categoria:\n",
        "        return \"Dinner\"\n",
        "    elif \"lodg\" in categoria:\n",
        "        return \"Lodge\"\n",
        "    elif \"motel\" in categoria:\n",
        "        return \"Motel\"\n",
        "    elif \"resort\" in categoria:\n",
        "        return \"Resort\"\n",
        "    elif \"inn\" in categoria:\n",
        "        return \"Inn\"\n",
        "    elif \"hotel\" in categoria:\n",
        "        return \"Hotel\"\n",
        "    else: return \"Otros\"\n",
        "\n",
        "def list_to_lower(my_list):\n",
        "    return [x.lower() for x in my_list]\n",
        "\n",
        "def dinamizar_lista_a_columna(df,serie):\n",
        "    lista_cat = df[serie].value_counts().index.to_list()\n",
        "    flat_list = [item for sublist in lista_cat for item in sublist]\n",
        "    for word in set(flat_list):\n",
        "        if pd.isna(word):\n",
        "            continue\n",
        "        else: \n",
        "            df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
        "            df[word].fillna(0,inplace=True)\n",
        "\n",
        "def get_state(texto):\n",
        "    if pd.isna(texto):\n",
        "        return None\n",
        "    else:\n",
        "        pattern = re.compile(r'(\\s{1}[A-Za-z]{2})(\\s{1}\\d{5})')\n",
        "        text = re.search(pattern,texto)\n",
        "        if bool(text):\n",
        "            return text.group(1)\n",
        "\n",
        "us_state_to_abbrev = {\n",
        "'ALASKA':'AK',\n",
        "'ARIZONA':'AZ',\n",
        "'ARKANSAS':'AR',\n",
        "'CALIFORNIA':'CA',\n",
        "'COLORADO':'CO',\n",
        "'CONNECTICUT':'CT',\n",
        "'DELAWARE':'DE',\n",
        "'FLORIDA':'FL',\n",
        "'GEORGIA':'GA',\n",
        "'HAWAII':'HI',\n",
        "'IDAHO':'ID',\n",
        "'ILLINOIS':'IL',\n",
        "'INDIANA':'IN',\n",
        "'IOWA':'IA',\n",
        "'KANSAS':'KS',\n",
        "'KENTUCKY':'KY',\n",
        "'LOUISIANA':'LA',\n",
        "'MAINE':'ME',\n",
        "'MARYLAND':'MD',\n",
        "'MASSACHUSETTS':'MA',\n",
        "'MICHIGAN':'MI',\n",
        "'MINNESOTA':'MN',\n",
        "'MISSISSIPPI':'MS',\n",
        "'MISSOURI':'MO',\n",
        "'MONTANA':'MT',\n",
        "'NEBRASKA':'NE',\n",
        "'NEVADA':'NV',\n",
        "'NEW HAMPSHIRE':'NH',\n",
        "'NEW JERSEY':'NJ',\n",
        "'NEW MEXICO':'NM',\n",
        "'NEW YORK':'NY',\n",
        "'NORTH CAROLINA':'NC',\n",
        "'NORTH DAKOTA':'ND',\n",
        "'OHIO':'OH',\n",
        "'OKLAHOMA':'OK',\n",
        "'OREGON':'OR',\n",
        "'PENNSYLVANIA':'PA',\n",
        "'RHODE ISLAND':'RI',\n",
        "'SOUTH CAROLINA':'SC',\n",
        "'SOUTH DAKOTA':'SD',\n",
        "'TENNESSEE':'TN',\n",
        "'TEXAS':'TX',\n",
        "'UTAH':'UT',\n",
        "'VERMONT':'VT',\n",
        "'VIRGINIA':'VA',\n",
        "'WASHINGTON':'WA',\n",
        "'WEST VIRGINIA':'WV',\n",
        "'WISCONSIN':'WI',\n",
        "'WYOMING':'WY',\n",
        "'ALABAMA':'AL',\n",
        "'DISTRICT OF COLUMBIA':'DC'\n",
        "}\n",
        "\n",
        "abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))\n",
        "\n",
        "def get_descrip_state(texto):\n",
        "    for i in us_state_to_abbrev.keys():\n",
        "        if i in texto:\n",
        "            return i\n",
        "            exit"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Segmentacion de datos para industr√≠a Hotelera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vamos a mapear todos los json, extraer todos los sitios con categoria hotel, motel, lodge, resort y los vamos acumular en una tabla para poder segmentar otros archivos\n",
        "#este proceso toma 4 minutos\n",
        "url_sitios = \"d:/Henry/Proyecto Final/Data/Sitios\"\n",
        "rutas_sitios = get_urls(url_sitios)\n",
        "df_gmap_id = get_gmap_id(rutas_sitios[\"ruta\"])\n",
        "df_gmap_id.drop_duplicates(subset=\"gmap_id\",inplace=True)\n",
        "df_gmap_id.reset_index(drop=True,inplace=True)\n",
        "df_gmap_id[\"gmap_id\"].to_csv(\"d:/Henry/Proyecto Final/Data/gmap_id.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carga incremental"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Codigo de carga para Sitios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#3 min para leer 11 jsons\n",
        "#cargar lista de filtros de categorias\n",
        "filtros = pd.read_csv(\"d:/Henry/Proyecto Final/Data/gmap_id.csv\",index_col=0, names=[\"gmap_id\"])\n",
        "#cargar lista de rutas ya cargadas\n",
        "rutas_sitios = pd.read_csv(\"d:/Henry/Proyecto Final/Data/rutas_sitios.csv\")\n",
        "#mapear carpetas para cargar\n",
        "url_sitios = \"d:/Henry/Proyecto Final/Data/Sitios\"\n",
        "df_url_sitios = get_urls(url_sitios)\n",
        "#para realizar una carga de solo las cosas nuevas, vamos a realizar una limpieza entre las url_cargadas vs las mapeadas y borrar duplicados, de esa forma solo pasamos lo nuevo.\n",
        "############ desactivar esta linea para realizar cargas iniciales #######################\n",
        "df_url_sitios = df_url_sitios[~df_url_sitios[\"ruta\"].isin(rutas_sitios[\"ruta\"])]\n",
        "#########################################################################################\n",
        "#si no encuentra rutas nuevas, vamos a detener el script\n",
        "if len(df_url_sitios) == 0:\n",
        "    sys.exit()\n",
        "#preparamos los datos para lo nuevo\n",
        "sitios_hoteles = get_reviews_url_info(df_url_sitios[\"ruta\"],filtros,True)\n",
        "#vamos a actualizar la tabla de repetidos, en base las nuevas rutas ya cargadas\n",
        "with open('d:/Henry/Proyecto Final/Data/rutas_sitios.csv', 'a', newline=\"\") as f:\n",
        "    df_url_sitios.to_csv(f, index=False, header=False)\n",
        "\n",
        "from google.cloud import storage\n",
        "path_to_private_key = 'd:/Henry/Proyecto Final/Carga incremental/genial-core-378003-6eb2af8349db.json'\n",
        "client = storage.Client.from_service_account_json(json_credentials_path=path_to_private_key)\n",
        "\n",
        "# The bucket on GCS in which to write the CSV file\n",
        "bucket = client.bucket('raw_data_vault')\n",
        "# The name assigned to the CSV file on GCS\n",
        "blob = bucket.blob('sitios_hoteles_{}.csv'.format(datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
        "blob.upload_from_string(sitios_hoteles.to_csv(), 'text/csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Codigo de carga de Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#21 min para revisar 51 estaods con 10json cada uno\n",
        "#cargar lista de filtros de categorias\n",
        "filtros = pd.read_csv(\"d:/Henry/Proyecto Final/Data/gmap_id.csv\",index_col=0, names=[\"gmap_id\"])\n",
        "#cargar lista de rutas ya cargadas\n",
        "rutas_reviews = pd.read_csv(\"d:/Henry/Proyecto Final/Data/rutas_reviews.csv\")\n",
        "#mapear carpetas para cargar\n",
        "url_reviews = \"d:/Henry/Proyecto Final/Data/Reviews\"\n",
        "df_url_reviews = get_urls(url_reviews)\n",
        "#para realizar una carga de solo las cosas nuevas, vamos a realizar una limpieza entre las url_cargadas vs las mapeadas y borrar duplicados, de esa forma solo pasamos lo nuevo.\n",
        "############ desactivar esta linea para realizar cargas iniciales #######################\n",
        "#df_url_reviews = df_url_reviews[~df_url_reviews[\"ruta\"].isin(rutas_reviews[\"ruta\"])]\n",
        "#########################################################################################\n",
        "#si no encuentra rutas nuevas, vamos a detener el script\n",
        "if len(df_url_reviews) == 0:\n",
        "    sys.exit()\n",
        "#preparamos los datos para lo nuevo\n",
        "reviews_hoteles = get_reviews_url_info(df_url_reviews[\"ruta\"],filtros,True)\n",
        "#vamos a actualizar la tabla de repetidos, en base las nuevas rutas ya cargadas\n",
        "with open('d:/Henry/Proyecto Final/Data/rutas_reviews.csv', 'a', newline=\"\") as f:\n",
        "    df_url_reviews.to_csv(f, index=False, header=False)\n",
        "\n",
        "from google.cloud import storage\n",
        "path_to_private_key = 'd:/Henry/Proyecto Final/Carga incremental/genial-core-378003-6eb2af8349db.json'\n",
        "client = storage.Client.from_service_account_json(json_credentials_path=path_to_private_key)\n",
        "\n",
        "# The bucket on GCS in which to write the CSV file\n",
        "bucket = client.bucket('raw_data_vault')\n",
        "# The name assigned to the CSV file on GCS\n",
        "blob = bucket.blob('reviews_hoteles_{}.csv'.format(datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
        "blob.upload_from_string(reviews_hoteles.to_csv(), 'text/csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparacion de datos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lectura de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c0AVTQsm6zHt"
      },
      "outputs": [],
      "source": [
        "## Se leen los dos archivos que tienen el filtro realizado de \"Industria Hotelera\"\n",
        "\n",
        "reviews_hotels = pd.read_csv(\"d:/Henry/Proyecto Final/Dataset_Consolidado/reviews_hoteles.csv\",index_col=0)\n",
        "sitios_hotels = pd.read_csv(\"d:/Henry/Proyecto Final/Dataset_Consolidado/sitios_hoteles.csv\",index_col=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepracion sitios_hotel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading formatted geocoded file...\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'title'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17532\\864376740.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"city_name\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"city_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'city_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcity_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4770\u001b[0m         \"\"\"\n\u001b[1;32m-> 4771\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4773\u001b[0m     def _reduce(\n",
            "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[1;31m# self.f is Callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1157\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17532\\864376740.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"city_name\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"city_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'city_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitios_hotels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcity_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'title'"
          ]
        }
      ],
      "source": [
        "#### tiempo ejecucion 3min ####\n",
        "#pasamos a mayus la dir\n",
        "sitios_hotels[\"address\"] = sitios_hotels[\"address\"].str.upper()\n",
        "#corremos la funcion de regular expression que sirve para encontrar_AA_#####, que seria a abreviatura y el codigo postal.\n",
        "sitios_hotels[\"state_xx\"] = sitios_hotels[\"address\"].apply(lambda x: get_state(x))\n",
        "#pasamos el diccionario para saber a que estado se refiere el proyecto.\n",
        "sitios_hotels[\"state_name\"] = sitios_hotels[\"state_xx\"].str.strip().map(abbrev_to_us_state)\n",
        "#para aquellos que s√≠ encontr√≥, vamos a sacar la ciudad.\n",
        "sitios_hotels[\"city_name\"] = sitios_hotels.apply(lambda row: np.nan if pd.isnull(row.state_xx) else (row.address.split(\",\")[2].strip() if len(row.address.split(\",\"))==4 else np.nan),axis=1)\n",
        "#para aquellos que no encontramos, vamos a correr la funcion para hallar segun lat y long\n",
        "##primero creamos la columnas de tuplas lat,long\n",
        "sitios_hotels[\"coordenadas\"] = list(zip(sitios_hotels[\"latitude\"],sitios_hotels[\"longitude\"]))\n",
        "sitios_hotels[\"reverse_coor\"] = sitios_hotels.apply(lambda row: rg.search(row.coordenadas) if pd.isnull(row.state_name) else np.nan, axis=1)\n",
        "sitios_hotels[\"state_name\"] = sitios_hotels.apply(lambda row: row.reverse_coor[0][\"admin1\"] if pd.notnull(row.reverse_coor) else row.state_name, axis=1)\n",
        "sitios_hotels[\"state_name\"] = sitios_hotels[\"state_name\"].str.upper()\n",
        "sitios_hotels[\"state_name\"] = sitios_hotels[\"state_name\"].str.replace(\"DISTRICT OF COLUMBIA\",\"WASHINGTON, D.C.\")\n",
        "sitios_hotels[\"city_name\"] = sitios_hotels.apply(lambda row: row.reverse_coor[0][\"admin2\"] if pd.notnull(row.reverse_coor) else row.city_name, axis=1)\n",
        "sitios_hotels[\"city_name\"] = sitios_hotels[\"city_name\"].str.upper()\n",
        "sitios_hotels['state_name'] = sitios_hotels.state_name.str.title()\n",
        "sitios_hotels['city_name'] = sitios_hotels.city_name.str.title()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#abrir columna MISC\n",
        "df_misc = sitios_hotels.dropna(subset=\"MISC\")\n",
        "df_misc_dinamizado = pd.json_normalize(df_misc[\"MISC\"].apply(only_dict).tolist())\n",
        "sitios_hotels = formatear_columnas(df_misc_dinamizado,sitios_hotels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#clasificar los hoteles\n",
        "sitios_hotels[\"category\"] = sitios_hotels[\"category\"].apply(only_dict).tolist()\n",
        "sitios_hotels[\"category\"] = sitios_hotels[\"category\"].apply(list_to_lower)\n",
        "sitios_hotels[\"cat_name\"] = sitios_hotels.name.apply(lambda x: reduccion_categoria(x.lower()) if pd.notnull(x) else x)\n",
        "sitios_hotels[\"cat_name\"] = sitios_hotels.apply(lambda row: reduccion_categoria(str(row.category)) if row.cat_name == \"Otros\" else row.cat_name, axis=1)\n",
        "sitios_hotels.drop(sitios_hotels[sitios_hotels[\"cat_name\"]==\"Dinner\"].index,inplace=True)\n",
        "sitios_hotels.reset_index(drop=True,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\1610941285.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[word] = df[serie].dropna().apply(lambda x: 1 if word in x else 0)\n"
          ]
        }
      ],
      "source": [
        "#abrir columnas creadas por MISC\n",
        "for i in df_misc_dinamizado.columns:\n",
        "    dinamizar_lista_a_columna(sitios_hotels,i)\n",
        "sitios_hotels.drop(df_misc_dinamizado.columns,axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "sitios_num_col = len(sitios_hotels.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.wheelchair\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.restaurant\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.bar\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.child_care\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.familiar/group\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.all_payment_method\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.lgtbq_friendly\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.outdoor_seating\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.breakfast\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.delivery\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.live_performance\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.covid\"] = \\\n",
            "C:\\Users\\wongp\\AppData\\Local\\Temp\\ipykernel_17532\\2465753634.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  sitios_hotels[\"t.fireplace\"] = \\\n"
          ]
        }
      ],
      "source": [
        "#Agrupacion de columnas por servicios importantes\n",
        "\n",
        "sitios_hotels[\"t.wheelchair\"] = \\\n",
        "    sitios_hotels[\"Wheelchair accessible elevator\"] + \\\n",
        "    sitios_hotels[\"Wheelchair accessible entrance\"] + \\\n",
        "    sitios_hotels[\"Wheelchair accessible parking lot\"] + \\\n",
        "    sitios_hotels[\"Wheelchair accessible restroom\"] + \\\n",
        "    sitios_hotels[\"Wheelchair accessible seating\"] + \\\n",
        "    sitios_hotels[\"Wheelchair-accessible car park\"] + \\\n",
        "    sitios_hotels[\"Wheelchair-accessible entrance\"] + \\\n",
        "    sitios_hotels[\"Wheelchair-accessible lift\"] + \\\n",
        "    sitios_hotels[\"Wheelchair-accessible seating\"] + \\\n",
        "    sitios_hotels[\"Wheelchair-accessible toilet\"]\n",
        "\n",
        "sitios_hotels[\"t.restaurant\"] = \\\n",
        "    sitios_hotels[\"All you can eat\"] + \\\n",
        "    sitios_hotels[\"Braille menu\"] + \\\n",
        "    sitios_hotels[\"Catering\"] + \\\n",
        "    sitios_hotels[\"Comfort food\"] + \\\n",
        "    sitios_hotels[\"Dessert\"] + \\\n",
        "    sitios_hotels[\"Dine-in\"] + \\\n",
        "    sitios_hotels[\"Dinner\"] + \\\n",
        "    sitios_hotels[\"Dinner reservations recommended\"] + \\\n",
        "    sitios_hotels[\"Drive-through\"] + \\\n",
        "    sitios_hotels[\"Fast service\"] + \\\n",
        "    sitios_hotels[\"Food\"] + \\\n",
        "    sitios_hotels[\"Great coffee\"] + \\\n",
        "    sitios_hotels[\"Great dessert\"] + \\\n",
        "    sitios_hotels[\"Great tea selection\"] + \\\n",
        "    sitios_hotels[\"Halal food\"] + \\\n",
        "    sitios_hotels[\"Happy hour food\"] + \\\n",
        "    sitios_hotels[\"Happy-hour food\"] + \\\n",
        "    sitios_hotels[\"Healthy options\"] + \\\n",
        "    sitios_hotels[\"In-store pickup\"] + \\\n",
        "    sitios_hotels[\"In-store shopping\"] + \\\n",
        "    sitios_hotels[\"Kids' menu\"] + \\\n",
        "    sitios_hotels[\"Late-night food\"] + \\\n",
        "    sitios_hotels[\"Lunch\"] + \\\n",
        "    sitios_hotels[\"Organic dishes\"] + \\\n",
        "    sitios_hotels[\"Outside food allowed\"] + \\\n",
        "    sitios_hotels[\"Quick bite\"] + \\\n",
        "    sitios_hotels[\"Restaurant\"] + \\\n",
        "    sitios_hotels[\"Small plates\"] + \\\n",
        "    sitios_hotels[\"Solo dining\"] + \\\n",
        "    sitios_hotels[\"Vegetarian options\"] \n",
        "\n",
        "sitios_hotels[\"t.bar\"] = \\\n",
        "    sitios_hotels[\"Alcohol\"] + \\\n",
        "    sitios_hotels[\"Bar games\"] + \\\n",
        "    sitios_hotels[\"Bar on site\"] + \\\n",
        "    sitios_hotels[\"Bar onsite\"] + \\\n",
        "    sitios_hotels[\"Beer\"] + \\\n",
        "    sitios_hotels[\"Cocktails\"] + \\\n",
        "    sitios_hotels[\"Food at bar\"] + \\\n",
        "    sitios_hotels[\"Great bar food\"] + \\\n",
        "    sitios_hotels[\"Great beer selection\"] + \\\n",
        "    sitios_hotels[\"Great cocktails\"] + \\\n",
        "    sitios_hotels[\"Great wine list\"] + \\\n",
        "    sitios_hotels[\"Happy hour drinks\"] + \\\n",
        "    sitios_hotels[\"Happy-hour drinks\"] + \\\n",
        "    sitios_hotels[\"Hard liquor\"] + \\\n",
        "    sitios_hotels[\"Salad bar\"] + \\\n",
        "    sitios_hotels[\"Spirits\"] + \\\n",
        "    sitios_hotels[\"Wine\"]\n",
        "\n",
        "sitios_hotels[\"t.child_care\"] = \\\n",
        "    sitios_hotels[\"Child care\"] + \\\n",
        "    sitios_hotels[\"Good for kids\"] + \\\n",
        "    sitios_hotels[\"High chairs\"]\n",
        "\n",
        "sitios_hotels[\"t.familiar/group\"] = \\\n",
        "    sitios_hotels[\"Family friendly\"] + \\\n",
        "    sitios_hotels[\"Family-friendly\"] + \\\n",
        "    sitios_hotels[\"Groups\"]\n",
        "\n",
        "sitios_hotels[\"t.all_payment_method\"] = \\\n",
        "    sitios_hotels[\"Checks\"] + \\\n",
        "    sitios_hotels[\"Cheques\"] + \\\n",
        "    sitios_hotels[\"Credit cards\"] + \\\n",
        "    sitios_hotels[\"Debit cards\"] + \\\n",
        "    sitios_hotels[\"Membership required\"] + \\\n",
        "    sitios_hotels[\"NFC mobile payments\"]\n",
        "\n",
        "sitios_hotels[\"t.lgtbq_friendly\"] = \\\n",
        "    sitios_hotels[\"Gender-neutral restroom\"] + \\\n",
        "    sitios_hotels[\"Gender-neutral toilets\"] + \\\n",
        "    sitios_hotels[\"LGBTQ friendly\"] + \\\n",
        "    sitios_hotels[\"LGBTQ-friendly\"] + \\\n",
        "    sitios_hotels[\"Transgender safespace\"]\n",
        "\n",
        "sitios_hotels[\"t.outdoor_seating\"] = \\\n",
        "    sitios_hotels[\"Outdoor seating\"] + \\\n",
        "    sitios_hotels[\"Rooftop seating\"] + \\\n",
        "    sitios_hotels[\"Seating\"] + \\\n",
        "    sitios_hotels[\"Stadium seating\"]\n",
        "\n",
        "sitios_hotels[\"t.breakfast\"] = \\\n",
        "    sitios_hotels[\"Breakfast\"] + \\\n",
        "    sitios_hotels[\"Coffee\"]\n",
        "\n",
        "sitios_hotels[\"t.delivery\"] = \\\n",
        "    sitios_hotels[\"Delivery\"] + \\\n",
        "    sitios_hotels[\"Takeaway\"] + \\\n",
        "    sitios_hotels[\"Takeout\"]\n",
        "\n",
        "sitios_hotels[\"t.live_performance\"] = \\\n",
        "    sitios_hotels[\"Live music\"] + \\\n",
        "    sitios_hotels[\"Live performances\"]\n",
        "\n",
        "sitios_hotels[\"t.covid\"] = \\\n",
        "    sitios_hotels[\"Mask required\"] + \\\n",
        "    sitios_hotels[\"No-contact delivery\"] + \\\n",
        "    sitios_hotels[\"Staff get temperature checks\"] + \\\n",
        "    sitios_hotels[\"Staff required to disinfect surfaces between visits\"] + \\\n",
        "    sitios_hotels[\"Staff wear masks\"] + \\\n",
        "    sitios_hotels[\"Temperature check required\"]\n",
        "\n",
        "sitios_hotels[\"t.fireplace\"] = \\\n",
        "    sitios_hotels[\"Fireplace\"]\n",
        "\n",
        "#dropeamos todas  las columnas que no vamos a usar para nada\n",
        "sitios_hotels.drop(sitios_hotels.iloc[:,23:sitios_num_col].columns,inplace=True,axis=1)\n",
        "sitios_hotels.drop_duplicates(subset=['name','address','latitude','longitude'],inplace=True)\n",
        "sitios_hotels.reset_index(inplace=True,drop=True)\n",
        "sitios_hotels.rename(columns={\"name\":\"name_hotel\"},inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "sitios_hotels.to_csv('d:/Henry/Proyecto Final/Dataset_Consolidado/sitios_hotel_para_Eda.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparacion reviews_hotels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OPBm7YXR6zH0"
      },
      "outputs": [],
      "source": [
        "# Se transforma la columna tiempo a fecha\n",
        "reviews_hotels[\"time\"]= pd.to_datetime(reviews_hotels[\"time\"],unit='ms').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "reviews_hotels[\"response\"] = reviews_hotels[\"resp\"].apply(lambda x: 0 if pd.isna(x) else 1 )\n",
        "reviews_hotels[\"state_from_json_url\"] = reviews_hotels[\"url_origen\"].apply(lambda x: x.split(\"/\")[5].split(\"-\")[1])\n",
        "reviews_hotels.drop_duplicates(subset=['name','time','text','rating','gmap_id','pics', 'resp'],inplace=True)\n",
        "reviews_hotels.reset_index(inplace=True,drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_hotels.to_csv('d:/Henry/Proyecto Final/Dataset_Consolidado/reviews_hotel_para_Eda.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
